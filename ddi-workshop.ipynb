{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Data Intelligence (DDI) Workshop Notebook\n",
    "\n",
    "> Aligns with slides: setup → lineage → quality → risk → governance.\n",
    "\n",
    "Objectives:\n",
    "- Build a small lineage graph of AI data assets\n",
    "- Compute basic quality metrics\n",
    "- Analyze risk propagation across the graph\n",
    "- Simulate simple governance rules and actions\n",
    "\n",
    "Sections:\n",
    "1. Setup and imports\n",
    "2. Build asset inventory and lineage\n",
    "3. Quality monitoring basics\n",
    "4. Risk propagation analysis\n",
    "5. Governance rules and automated actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages: ['numpy==1.24.3']\n",
      "Installing packages: ['numpy==1.24.3']\n",
      "Installing packages: ['numpy==1.24.3']\n"
     ]
    }
   ],
   "source": [
    "# 1) Setup and imports\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from governance_tools.data_lineage import (\n",
    "    DataLineageTracker, DataAsset, DataAssetType, RiskLevel\n",
    ")\n",
    "\n",
    "# Optional: configure data paths\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data-samples')\n",
    "print('Data dir:', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLineageTracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 2) Build asset inventory and lineage\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tracker \u001b[38;5;241m=\u001b[39m DataLineageTracker()\n\u001b[0;32m      4\u001b[0m now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m      6\u001b[0m assets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     DataAsset(\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_raw_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer Raw Data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     ),\n\u001b[0;32m     35\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataLineageTracker' is not defined"
     ]
    }
   ],
   "source": [
    "# 2) Build asset inventory and lineage\n",
    "tracker = DataLineageTracker()\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "assets = [\n",
    "    DataAsset(\n",
    "        id=\"customer_raw_data\", name=\"Customer Raw Data\",\n",
    "        type=DataAssetType.SOURCE, source_system=\"CRM\",\n",
    "        created_at=now - timedelta(days=30), last_updated=now - timedelta(days=1),\n",
    "        quality_score=0.85, risk_level=RiskLevel.MEDIUM,\n",
    "        metadata={\"format\": \"CSV\", \"size_gb\": 2.5},\n",
    "    ),\n",
    "    DataAsset(\n",
    "        id=\"customer_cleaned_data\", name=\"Customer Cleaned Data\",\n",
    "        type=DataAssetType.TRANSFORMATION, source_system=\"Data Pipeline\",\n",
    "        created_at=now - timedelta(days=29), last_updated=now - timedelta(hours=6),\n",
    "        quality_score=0.92, risk_level=RiskLevel.LOW,\n",
    "        metadata={\"transformation\": \"data_cleaning_v2\"},\n",
    "    ),\n",
    "    DataAsset(\n",
    "        id=\"churn_prediction_model\", name=\"Customer Churn Prediction Model\",\n",
    "        type=DataAssetType.MODEL, source_system=\"ML Platform\",\n",
    "        created_at=now - timedelta(days=7), last_updated=now - timedelta(hours=12),\n",
    "        quality_score=0.88, risk_level=RiskLevel.HIGH,\n",
    "        metadata={\"model_type\": \"RandomForest\", \"accuracy\": 0.94},\n",
    "    ),\n",
    "    DataAsset(\n",
    "        id=\"churn_predictions\", name=\"Churn Predictions\",\n",
    "        type=DataAssetType.OUTPUT, source_system=\"Serving\",\n",
    "        created_at=now - timedelta(days=2), last_updated=now - timedelta(hours=1),\n",
    "        quality_score=0.90, risk_level=RiskLevel.MEDIUM,\n",
    "        metadata={\"sla\": \"99.9%\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "for a in assets:\n",
    "    tracker.add_asset(a)\n",
    "\n",
    "tracker.add_dependency(\"customer_raw_data\", \"customer_cleaned_data\", \"data_cleaning\", 0.95)\n",
    "tracker.add_dependency(\"customer_cleaned_data\", \"churn_prediction_model\", \"model_training\", 0.90)\n",
    "tracker.add_dependency(\"churn_prediction_model\", \"churn_predictions\", \"inference\", 0.98)\n",
    "\n",
    "print(f\"Assets: {len(tracker.assets)} | Dependencies: {tracker.graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Simulated dataset quality metrics for assets\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m quality_metrics \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[0;32m      6\u001b[0m     {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masset_id\u001b[39m\u001b[38;5;124m'\u001b[39m: a\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompleteness\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mclip(a\u001b[38;5;241m.\u001b[39mquality_score \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidity\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mclip(a\u001b[38;5;241m.\u001b[39mquality_score \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsistency\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mclip(a\u001b[38;5;241m.\u001b[39mquality_score \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeliness\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mclip(a\u001b[38;5;241m.\u001b[39mquality_score \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     12\u001b[0m     }\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m assets\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m quality_thresholds \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompleteness\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidity\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.90\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsistency\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.85\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeliness\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.90\u001b[39m,\n\u001b[0;32m     21\u001b[0m }\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_thresholds\u001b[39m(row, thresholds):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 3) Quality monitoring basics\n",
    "import numpy as np\n",
    "\n",
    "# Simulated dataset quality metrics for assets\n",
    "quality_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'asset_id': a.id,\n",
    "        'completeness': np.clip(a.quality_score + np.random.normal(0, 0.05), 0, 1),\n",
    "        'validity': np.clip(a.quality_score + np.random.normal(0, 0.05), 0, 1),\n",
    "        'consistency': np.clip(a.quality_score + np.random.normal(0, 0.05), 0, 1),\n",
    "        'timeliness': np.clip(a.quality_score + np.random.normal(0, 0.05), 0, 1),\n",
    "    }\n",
    "    for a in assets\n",
    "])\n",
    "\n",
    "quality_thresholds = {\n",
    "    'completeness': 0.95,\n",
    "    'validity': 0.90,\n",
    "    'consistency': 0.85,\n",
    "    'timeliness': 0.90,\n",
    "}\n",
    "\n",
    "def check_thresholds(row, thresholds):\n",
    "    violations = []\n",
    "    for k, v in thresholds.items():\n",
    "        if row[k] < v:\n",
    "            violations.append(f\"{k}<{v}\")\n",
    "    return violations\n",
    "\n",
    "quality_metrics['violations'] = quality_metrics.apply(lambda r: check_thresholds(r, quality_thresholds), axis=1)\n",
    "display(quality_metrics)\n",
    "print(\"Assets below thresholds:\")\n",
    "display(quality_metrics[quality_metrics['violations'].map(len) > 0][['asset_id', 'violations']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Risk propagation analysis\n",
    "source_asset_id = 'customer_raw_data'\n",
    "risk_scores = tracker.calculate_risk_propagation(source_asset_id)\n",
    "\n",
    "print('Risk scores from', source_asset_id)\n",
    "for k, v in risk_scores.items():\n",
    "    print(f\"  {k}: {v:.3f}\")\n",
    "\n",
    "report = tracker.generate_lineage_report()\n",
    "print('\\nLineage Report Summary:')\n",
    "print('- Total assets:', report['total_assets'])\n",
    "print('- Total dependencies:', report['total_dependencies'])\n",
    "print('- Average quality score:', f\"{report['quality_summary']['average_quality']:.2f}\")\n",
    "print('- Assets below quality threshold:', len(report['quality_summary']['below_threshold_assets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Governance rules and automated actions\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class RuleResult:\n",
    "    asset_id: str\n",
    "    rule: str\n",
    "    passed: bool\n",
    "    details: Dict\n",
    "\n",
    "def evaluate_governance_rules(assets: List[DataAsset], quality_df: pd.DataFrame) -> List[RuleResult]:\n",
    "    results: List[RuleResult] = []\n",
    "    quality_by_asset = quality_df.set_index('asset_id').to_dict(orient='index')\n",
    "    for a in assets:\n",
    "        q = quality_by_asset.get(a.id, {})\n",
    "        # Rule 1: Quality threshold gate\n",
    "        overall_ok = (q.get('completeness', 1) >= 0.95 and\n",
    "                      q.get('validity', 1) >= 0.90 and\n",
    "                      q.get('consistency', 1) >= 0.85 and\n",
    "                      q.get('timeliness', 1) >= 0.90)\n",
    "        results.append(RuleResult(a.id, 'QualityThresholds', overall_ok, q))\n",
    "        # Rule 2: High-risk assets must have high quality\n",
    "        if a.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]:\n",
    "            high_risk_ok = a.quality_score >= 0.90\n",
    "            results.append(RuleResult(a.id, 'HighRiskQuality>=0.90', high_risk_ok, {'quality_score': a.quality_score}))\n",
    "    return results\n",
    "\n",
    "def remediation_actions(result: RuleResult):\n",
    "    if result.rule == 'QualityThresholds' and not result.passed:\n",
    "        return ['quarantine_dataset', 'notify_owner', 'open_incident']\n",
    "    if result.rule == 'HighRiskQuality>=0.90' and not result.passed:\n",
    "        return ['increase_sampling', 'trigger_retraining', 'add_monitoring']\n",
    "    return []\n",
    "\n",
    "results = evaluate_governance_rules(assets, quality_metrics)\n",
    "violations = [r for r in results if not r.passed]\n",
    "print(f\"Violations: {len(violations)}\")\n",
    "for v in violations:\n",
    "    print(f\"- {v.asset_id} failed {v.rule} -> actions: {remediation_actions(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Data Intelligence Workshop\n",
    "\n",
    "## Enabling Proactive Governance and Risk Management in AI Systems\n",
    "\n",
    "Welcome to this hands-on workshop on Dynamic Data Intelligence (DDI). We'll explore practical techniques for understanding, assessing, and managing data risks in AI systems.\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How to track data lineage and dependencies\n",
    "2. Implementing dynamic quality assessment\n",
    "3. Risk propagation analysis across data pipelines\n",
    "4. Building proactive governance frameworks\n",
    "5. Real-world case studies and solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "First, let's start our infrastructure:\n",
    "\n",
    "```bash\n",
    "# In your terminal, run:\n",
    "docker-compose up -d\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This will start:\n",
    "- PostgreSQL (localhost:5432) - metadata storage\n",
    "- DataHub (http://localhost:9002) - data catalog\n",
    "- Redis (localhost:6379) - caching layer\n",
    "- Jaeger (http://localhost:16686) - observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Alison Cossette\\AppData\\Local\\Temp\\ipykernel_6324\\64816625.py\", line 4, in <module>\n",
      "    from governance_tools.data_lineage import DataLineageTracker, DataAsset, DataAssetType, RiskLevel\n",
      "  File \"c:\\Users\\Alison Cossette\\OneDrive\\GitHub\\hidden-patterns-agentic-ai-talk\\dynamic-data-intelligence-talk\\governance_tools\\data_lineage.py\", line 7, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Alison Cossette\\AppData\\Local\\Temp\\ipykernel_6324\\64816625.py\", line 4, in <module>\n",
      "    from governance_tools.data_lineage import DataLineageTracker, DataAsset, DataAssetType, RiskLevel\n",
      "  File \"c:\\Users\\Alison Cossette\\OneDrive\\GitHub\\hidden-patterns-agentic-ai-talk\\dynamic-data-intelligence-talk\\governance_tools\\data_lineage.py\", line 7, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.5 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import our DDI modules\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgovernance_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_lineage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLineageTracker, DataAsset, DataAssetType, RiskLevel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alison Cossette\\OneDrive\\GitHub\\hidden-patterns-agentic-ai-talk\\dynamic-data-intelligence-talk\\governance_tools\\data_lineage.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mDynamic Data Intelligence - Data Lineage Tracking\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTracks data flow and dependencies across AI systems\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Tuple\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Public\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Import our DDI modules\n",
    "import sys\n",
    "\n",
    "from governance_tools.data_lineage import DataLineageTracker, DataAsset, DataAssetType, RiskLevel\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print('✅ DDI modules loaded successfully!')\n",
    "print('📊 Ready for Dynamic Data Intelligence exploration')\n",
    "print('🔍 Data lineage tracking enabled')\n",
    "print('⚠️ Risk assessment tools ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Data Assets\n",
    "\n",
    "Let's start by creating and cataloging different types of data assets in our AI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our DDI system\n",
    "ddi_tracker = DataLineageTracker()\n",
    "\n",
    "# Create different types of data assets\n",
    "print('🏗️ Creating sample data assets...')\n",
    "\n",
    "# Raw data source\n",
    "customer_data = DataAsset(\n",
    "    id=\"customer_transactions\",\n",
    "    name=\"Customer Transaction Data\",\n",
    "    type=DataAssetType.SOURCE,\n",
    "    source_system=\"Banking Core\",\n",
    "    created_at=datetime.now() - timedelta(days=90),\n",
    "    last_updated=datetime.now() - timedelta(hours=2),\n",
    "    quality_score=0.78,  # Below our threshold\n",
    "    risk_level=RiskLevel.HIGH,  # Contains PII\n",
    "    metadata={\n",
    "        \"format\": \"JSON\",\n",
    "        \"size_gb\": 15.2,\n",
    "        \"contains_pii\": True,\n",
    "        \"update_frequency\": \"hourly\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cleaned/processed data\n",
    "processed_data = DataAsset(\n",
    "    id=\"processed_transactions\",\n",
    "    name=\"Processed Transaction Features\",\n",
    "    type=DataAssetType.TRANSFORMATION,\n",
    "    source_system=\"Data Pipeline\",\n",
    "    created_at=datetime.now() - timedelta(days=89),\n",
    "    last_updated=datetime.now() - timedelta(hours=2),\n",
    "    quality_score=0.92,\n",
    "    risk_level=RiskLevel.MEDIUM,\n",
    "    metadata={\n",
    "        \"transformation\": \"feature_engineering_v3\",\n",
    "        \"features_count\": 47,\n",
    "        \"anonymized\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# ML Model\n",
    "fraud_model = DataAsset(\n",
    "    id=\"fraud_detection_model\",\n",
    "    name=\"Real-time Fraud Detection Model\",\n",
    "    type=DataAssetType.MODEL,\n",
    "    source_system=\"ML Platform\",\n",
    "    created_at=datetime.now() - timedelta(days=14),\n",
    "    last_updated=datetime.now() - timedelta(days=3),\n",
    "    quality_score=0.94,\n",
    "    risk_level=RiskLevel.CRITICAL,  # Business critical\n",
    "    metadata={\n",
    "        \"model_type\": \"XGBoost\",\n",
    "        \"f1_score\": 0.87,\n",
    "        \"precision\": 0.91,\n",
    "        \"recall\": 0.84\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model outputs\n",
    "fraud_scores = DataAsset(\n",
    "    id=\"fraud_risk_scores\",\n",
    "    name=\"Real-time Fraud Risk Scores\",\n",
    "    type=DataAssetType.OUTPUT,\n",
    "    source_system=\"ML Platform\",\n",
    "    created_at=datetime.now() - timedelta(days=14),\n",
    "    last_updated=datetime.now() - timedelta(minutes=5),\n",
    "    quality_score=0.89,\n",
    "    risk_level=RiskLevel.HIGH,\n",
    "    metadata={\n",
    "        \"output_format\": \"real_time_stream\",\n",
    "        \"latency_ms\": 45,\n",
    "        \"throughput_tps\": 1200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add all assets to our tracker\n",
    "for asset in [customer_data, processed_data, fraud_model, fraud_scores]:\n",
    "    ddi_tracker.add_asset(asset)\n",
    "    print(f'   📦 Added: {asset.name} (Risk: {asset.risk_level.value}, Quality: {asset.quality_score:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building Data Lineage\n",
    "\n",
    "Now let's connect our assets to understand how data flows through our AI system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🔗 Building data lineage connections...')\n",
    "\n",
    "# Define the data flow\n",
    "ddi_tracker.add_dependency(\n",
    "    \"customer_transactions\", \n",
    "    \"processed_transactions\", \n",
    "    \"data_cleaning_and_feature_engineering\", \n",
    "    0.95\n",
    ")\n",
    "\n",
    "ddi_tracker.add_dependency(\n",
    "    \"processed_transactions\", \n",
    "    \"fraud_detection_model\", \n",
    "    \"model_training_and_validation\", \n",
    "    0.92\n",
    ")\n",
    "\n",
    "ddi_tracker.add_dependency(\n",
    "    \"fraud_detection_model\", \n",
    "    \"fraud_risk_scores\", \n",
    "    \"real_time_inference\", \n",
    "    0.98\n",
    ")\n",
    "\n",
    "print('✅ Data lineage established!')\n",
    "print(f'📊 Total assets: {len(ddi_tracker.assets)}')\n",
    "print(f'🔗 Total dependencies: {ddi_tracker.graph.number_of_edges()}')\n",
    "\n",
    "# Let's visualize the lineage\n",
    "print('\\n🗺️ Data Flow Visualization:')\n",
    "print('Customer Transactions → Processed Features → ML Model → Fraud Scores')\n",
    "print('     (Risk: HIGH)    →    (Risk: MED)    → (Risk: CRIT) → (Risk: HIGH)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Risk Propagation Analysis\n",
    "\n",
    "Let's analyze how data quality issues and risks propagate through our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('⚠️ Analyzing Risk Propagation...')\n",
    "\n",
    "# Analyze risk propagation from our source data\n",
    "source_risks = ddi_tracker.calculate_risk_propagation(\"customer_transactions\")\n",
    "\n",
    "print('\\n📈 Risk Propagation from Customer Transactions:')\n",
    "for asset_id, risk_score in source_risks.items():\n",
    "    asset = ddi_tracker.assets[asset_id]\n",
    "    print(f'   {asset.name}: {risk_score:.3f} (Quality: {asset.quality_score:.2f})')\n",
    "\n",
    "# Find what assets would be impacted if our source data had issues\n",
    "print('\\n🎯 Impact Analysis:')\n",
    "downstream_impacts = ddi_tracker.get_downstream_impacts(\"customer_transactions\")\n",
    "print(f'   Assets impacted by customer transaction issues: {len(downstream_impacts)}')\n",
    "\n",
    "for asset_id in downstream_impacts:\n",
    "    asset = ddi_tracker.assets[asset_id]\n",
    "    print(f'   - {asset.name} ({asset.type.value})')\n",
    "\n",
    "# Quality impact analysis\n",
    "quality_impact = ddi_tracker.get_quality_impact_analysis(\"customer_transactions\")\n",
    "print(f'\\n📉 Quality Impact Analysis:')\n",
    "print(f'   Quality degradation risk: {quality_impact[\"quality_degradation_risk\"]:.3f}')\n",
    "print(f'   Models potentially affected: {len(quality_impact[\"impacted_models\"])}')\n",
    "print(f'   Outputs potentially affected: {len(quality_impact[\"impacted_outputs\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dynamic Quality Monitoring\n",
    "\n",
    "Let's implement real-time quality monitoring and alerting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DynamicQualityMonitor:\n",
    "    def __init__(self, lineage_tracker):\n",
    "        self.tracker = lineage_tracker\n",
    "        self.quality_thresholds = {\n",
    "            'completeness': 0.95,\n",
    "            'validity': 0.90,\n",
    "            'consistency': 0.85,\n",
    "            'timeliness': 0.90\n",
    "        }\n",
    "        self.alerts = []\n",
    "    \n",
    "    def simulate_quality_check(self, asset_id):\n",
    "        \"\"\"Simulate a quality assessment\"\"\"\n",
    "        if asset_id not in self.tracker.assets:\n",
    "            return None\n",
    "        \n",
    "        # Simulate quality metrics (in real system, these would come from actual data profiling)\n",
    "        quality_metrics = {\n",
    "            'completeness': random.uniform(0.8, 1.0),\n",
    "            'validity': random.uniform(0.75, 0.98),\n",
    "            'consistency': random.uniform(0.70, 0.95),\n",
    "            'timeliness': random.uniform(0.85, 1.0)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)\n",
    "        \n",
    "        # Check for threshold violations\n",
    "        violations = []\n",
    "        for metric, score in quality_metrics.items():\n",
    "            if score < self.quality_thresholds[metric]:\n",
    "                violations.append(f'{metric}: {score:.3f} < {self.quality_thresholds[metric]}')\n",
    "        \n",
    "        # Generate alerts if needed\n",
    "        if violations:\n",
    "            alert = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'asset_id': asset_id,\n",
    "                'asset_name': self.tracker.assets[asset_id].name,\n",
    "                'violations': violations,\n",
    "                'overall_quality': overall_quality,\n",
    "                'downstream_impact': len(self.tracker.get_downstream_impacts(asset_id))\n",
    "            }\n",
    "            self.alerts.append(alert)\n",
    "        \n",
    "        return {\n",
    "            'asset_id': asset_id,\n",
    "            'metrics': quality_metrics,\n",
    "            'overall_quality': overall_quality,\n",
    "            'violations': violations\n",
    "        }\n",
    "    \n",
    "    def monitor_all_assets(self):\n",
    "        \"\"\"Run quality checks on all assets\"\"\"\n",
    "        results = []\n",
    "        for asset_id in self.tracker.assets.keys():\n",
    "            result = self.simulate_quality_check(asset_id)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "# Initialize quality monitoring\n",
    "monitor = DynamicQualityMonitor(ddi_tracker)\n",
    "\n",
    "print('🔍 Running Dynamic Quality Assessment...')\n",
    "quality_results = monitor.monitor_all_assets()\n",
    "\n",
    "for result in quality_results:\n",
    "    asset = ddi_tracker.assets[result['asset_id']]\n",
    "    print(f'\\n📊 {asset.name}:')\n",
    "    print(f'   Overall Quality: {result[\"overall_quality\"]:.3f}')\n",
    "    \n",
    "    if result['violations']:\n",
    "        print('   ⚠️ Quality Issues:')\n",
    "        for violation in result['violations']:\n",
    "            print(f'      - {violation}')\n",
    "    else:\n",
    "        print('   ✅ All quality thresholds met')\n",
    "\n",
    "# Show alerts\n",
    "if monitor.alerts:\n",
    "    print('\\n🚨 QUALITY ALERTS:')\n",
    "    for alert in monitor.alerts:\n",
    "        print(f'   Alert for {alert[\"asset_name\"]}:')\n",
    "        print(f'   - {len(alert[\"violations\"])} violations found')\n",
    "        print(f'   - {alert[\"downstream_impact\"]} assets potentially impacted')\n",
    "else:\n",
    "    print('\\n✅ No critical quality issues detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Proactive Risk Management\n",
    "\n",
    "Let's implement proactive risk management strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProactiveRiskManager:\n",
    "    def __init__(self, lineage_tracker, quality_monitor):\n",
    "        self.tracker = lineage_tracker\n",
    "        self.monitor = quality_monitor\n",
    "        self.risk_mitigation_strategies = {\n",
    "            'data_backup': 'Create backup of critical data sources',\n",
    "            'model_fallback': 'Use fallback model with different data sources',\n",
    "            'quality_filtering': 'Filter out low-quality data points',\n",
    "            'alert_stakeholders': 'Notify relevant stakeholders immediately',\n",
    "            'circuit_breaker': 'Temporarily halt processing to prevent propagation'\n",
    "        }\n",
    "    \n",
    "    def assess_system_risk(self):\n",
    "        \"\"\"Assess overall system risk\"\"\"\n",
    "        risk_assessment = {\n",
    "            'high_risk_assets': [],\n",
    "            'quality_concerns': [],\n",
    "            'critical_paths': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Identify high-risk assets\n",
    "        for asset_id, asset in self.tracker.assets.items():\n",
    "            if asset.risk_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]:\n",
    "                downstream_count = len(self.tracker.get_downstream_impacts(asset_id))\n",
    "                risk_assessment['high_risk_assets'].append({\n",
    "                    'asset': asset.name,\n",
    "                    'risk_level': asset.risk_level.value,\n",
    "                    'quality_score': asset.quality_score,\n",
    "                    'downstream_impact': downstream_count\n",
    "                })\n",
    "        \n",
    "        # Identify quality concerns\n",
    "        for asset_id, asset in self.tracker.assets.items():\n",
    "            if asset.quality_score < 0.85:\n",
    "                risk_assessment['quality_concerns'].append({\n",
    "                    'asset': asset.name,\n",
    "                    'quality_score': asset.quality_score,\n",
    "                    'risk_level': asset.risk_level.value\n",
    "                })\n",
    "        \n",
    "        # Find critical paths\n",
    "        critical_paths = self.tracker.identify_critical_paths()\n",
    "        risk_assessment['critical_paths'] = critical_paths\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if risk_assessment['high_risk_assets']:\n",
    "            risk_assessment['recommendations'].append(\n",
    "                'Implement additional monitoring for high-risk assets'\n",
    "            )\n",
    "        \n",
    "        if risk_assessment['quality_concerns']:\n",
    "            risk_assessment['recommendations'].append(\n",
    "                'Improve data quality processes for low-scoring assets'\n",
    "            )\n",
    "        \n",
    "        if critical_paths:\n",
    "            risk_assessment['recommendations'].append(\n",
    "                'Create redundant data paths for critical flows'\n",
    "            )\n",
    "        \n",
    "        return risk_assessment\n",
    "    \n",
    "    def simulate_incident(self, asset_id):\n",
    "        \"\"\"Simulate a data incident and recommended response\"\"\"\n",
    "        if asset_id not in self.tracker.assets:\n",
    "            return None\n",
    "        \n",
    "        asset = self.tracker.assets[asset_id]\n",
    "        downstream = self.tracker.get_downstream_impacts(asset_id)\n",
    "        \n",
    "        incident = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'affected_asset': asset.name,\n",
    "            'incident_type': 'data_quality_degradation',\n",
    "            'severity': 'HIGH' if len(downstream) > 2 else 'MEDIUM',\n",
    "            'impact_radius': downstream,\n",
    "            'recommended_actions': []\n",
    "        }\n",
    "        \n",
    "        # Generate recommended actions based on asset type and impact\n",
    "        if asset.type == DataAssetType.SOURCE:\n",
    "            incident['recommended_actions'].extend([\n",
    "                self.risk_mitigation_strategies['data_backup'],\n",
    "                self.risk_mitigation_strategies['alert_stakeholders']\n",
    "            ])\n",
    "        \n",
    "        if asset.type == DataAssetType.MODEL:\n",
    "            incident['recommended_actions'].extend([\n",
    "                self.risk_mitigation_strategies['model_fallback'],\n",
    "                self.risk_mitigation_strategies['circuit_breaker']\n",
    "            ])\n",
    "        \n",
    "        if len(downstream) > 2:\n",
    "            incident['recommended_actions'].append(\n",
    "                self.risk_mitigation_strategies['quality_filtering']\n",
    "            )\n",
    "        \n",
    "        return incident\n",
    "\n",
    "# Initialize risk manager\n",
    "risk_manager = ProactiveRiskManager(ddi_tracker, monitor)\n",
    "\n",
    "print('🛡️ Conducting System Risk Assessment...')\n",
    "risk_assessment = risk_manager.assess_system_risk()\n",
    "\n",
    "print('\\n📊 RISK ASSESSMENT RESULTS:')\n",
    "print(f'   High-risk assets: {len(risk_assessment[\"high_risk_assets\"])}')\n",
    "print(f'   Quality concerns: {len(risk_assessment[\"quality_concerns\"])}')\n",
    "print(f'   Critical paths: {len(risk_assessment[\"critical_paths\"])}')\n",
    "\n",
    "if risk_assessment['high_risk_assets']:\n",
    "    print('\\n⚠️ High-Risk Assets:')\n",
    "    for asset in risk_assessment['high_risk_assets']:\n",
    "        print(f'   - {asset[\"asset\"]} (Risk: {asset[\"risk_level\"]}, Quality: {asset[\"quality_score\"]:.2f})')\n",
    "\n",
    "if risk_assessment['recommendations']:\n",
    "    print('\\n💡 Recommendations:')\n",
    "    for i, rec in enumerate(risk_assessment['recommendations'], 1):\n",
    "        print(f'   {i}. {rec}')\n",
    "\n",
    "# Simulate an incident\n",
    "print('\\n🚨 INCIDENT SIMULATION:')\n",
    "incident = risk_manager.simulate_incident('customer_transactions')\n",
    "if incident:\n",
    "    print(f'   Incident: {incident[\"incident_type\"]} in {incident[\"affected_asset\"]}')\n",
    "    print(f'   Severity: {incident[\"severity\"]}')\n",
    "    print(f'   Assets at risk: {len(incident[\"impact_radius\"])}')\n",
    "    print('   Recommended Actions:')\n",
    "    for action in incident['recommended_actions']:\n",
    "        print(f'   - {action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comprehensive DDI Report\n",
    "\n",
    "Let's generate a comprehensive Dynamic Data Intelligence report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive lineage report\n",
    "lineage_report = ddi_tracker.generate_lineage_report()\n",
    "\n",
    "print('📋 DYNAMIC DATA INTELLIGENCE REPORT')\n",
    "print('=' * 50)\n",
    "print(f'Generated: {lineage_report[\"generated_at\"]}')\n",
    "\n",
    "print('\\n📊 SYSTEM OVERVIEW:')\n",
    "print(f'   Total Data Assets: {lineage_report[\"total_assets\"]}')\n",
    "print(f'   Total Dependencies: {lineage_report[\"total_dependencies\"]}')\n",
    "print(f'   Average Quality Score: {lineage_report[\"quality_summary\"][\"average_quality\"]:.3f}')\n",
    "\n",
    "print('\\n🏗️ ASSET TYPE DISTRIBUTION:')\n",
    "for asset_type, count in lineage_report['asset_types'].items():\n",
    "    print(f'   {asset_type.title()}: {count}')\n",
    "\n",
    "print('\\n⚠️ RISK DISTRIBUTION:')\n",
    "for risk_level, count in lineage_report['risk_distribution'].items():\n",
    "    print(f'   {risk_level.title()}: {count}')\n",
    "\n",
    "if lineage_report['quality_summary']['below_threshold_assets']:\n",
    "    print('\\n🔍 QUALITY CONCERNS:')\n",
    "    for asset in lineage_report['quality_summary']['below_threshold_assets']:\n",
    "        print(f'   - {asset[\"name\"]}: Quality {asset[\"quality_score\"]:.3f}')\n",
    "\n",
    "if lineage_report['critical_paths']:\n",
    "    print('\\n🛤️ CRITICAL DATA PATHS:')\n",
    "    for i, path in enumerate(lineage_report['critical_paths'], 1):\n",
    "        path_names = [ddi_tracker.assets[asset_id].name for asset_id in path]\n",
    "        print(f'   Path {i}: {\": → \".join(path_names)}')\n",
    "\n",
    "print('\\n💡 KEY INSIGHTS:')\n",
    "insights = []\n",
    "\n",
    "if lineage_report['quality_summary']['average_quality'] < 0.85:\n",
    "    insights.append('Overall system quality is below recommended threshold')\n",
    "\n",
    "high_risk_count = lineage_report['risk_distribution'].get('high', 0) + lineage_report['risk_distribution'].get('critical', 0)\n",
    "if high_risk_count > 0:\n",
    "    insights.append(f'{high_risk_count} assets classified as high or critical risk')\n",
    "\n",
    "if lineage_report['critical_paths']:\n",
    "    insights.append(f'{len(lineage_report[\"critical_paths\"])} critical data paths require attention')\n",
    "\n",
    "if not insights:\n",
    "    insights.append('System appears to be operating within acceptable parameters')\n",
    "\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f'   {i}. {insight}')\n",
    "\n",
    "print('\\n🎯 RECOMMENDED ACTIONS:')\n",
    "actions = [\n",
    "    'Implement continuous quality monitoring for all data sources',\n",
    "    'Establish automated alerts for quality threshold violations',\n",
    "    'Create backup data sources for critical assets',\n",
    "    'Regular review and update of risk classifications',\n",
    "    'Implement data validation at ingestion points'\n",
    "]\n",
    "\n",
    "for i, action in enumerate(actions, 1):\n",
    "    print(f'   {i}. {action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Analytics\n",
    "\n",
    "Let's explore some advanced DDI analytics techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_data_centrality(tracker):\n",
    "    \"\"\"Analyze which data assets are most central to the system\"\"\"\n",
    "    \n",
    "    # Calculate different centrality measures\n",
    "    centrality_measures = {}\n",
    "    \n",
    "    # Degree centrality (number of connections)\n",
    "    degree_centrality = nx.degree_centrality(tracker.graph)\n",
    "    \n",
    "    # Betweenness centrality (how often a node appears on shortest paths)\n",
    "    betweenness_centrality = nx.betweenness_centrality(tracker.graph)\n",
    "    \n",
    "    # PageRank (importance based on connections)\n",
    "    pagerank = nx.pagerank(tracker.graph)\n",
    "    \n",
    "    print('🎯 DATA ASSET CENTRALITY ANALYSIS:')\n",
    "    print('\\nMost Connected Assets (Degree Centrality):')\n",
    "    sorted_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    for asset_id, score in sorted_degree:\n",
    "        asset_name = tracker.assets[asset_id].name\n",
    "        print(f'   {asset_name}: {score:.3f}')\n",
    "    \n",
    "    print('\\nMost Critical Path Assets (Betweenness Centrality):')\n",
    "    sorted_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    for asset_id, score in sorted_betweenness:\n",
    "        asset_name = tracker.assets[asset_id].name\n",
    "        print(f'   {asset_name}: {score:.3f}')\n",
    "    \n",
    "    print('\\nMost Influential Assets (PageRank):')\n",
    "    sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "    for asset_id, score in sorted_pagerank:\n",
    "        asset_name = tracker.assets[asset_id].name\n",
    "        print(f'   {asset_name}: {score:.3f}')\n",
    "    \n",
    "    return {\n",
    "        'degree': degree_centrality,\n",
    "        'betweenness': betweenness_centrality,\n",
    "        'pagerank': pagerank\n",
    "    }\n",
    "\n",
    "def simulate_cascading_failure(tracker, failed_asset_id):\n",
    "    \"\"\"Simulate what happens when an asset fails\"\"\"\n",
    "    if failed_asset_id not in tracker.assets:\n",
    "        return None\n",
    "    \n",
    "    failed_asset = tracker.assets[failed_asset_id]\n",
    "    \n",
    "    # Get all downstream assets that would be affected\n",
    "    affected_assets = tracker.get_downstream_impacts(failed_asset_id)\n",
    "    \n",
    "    # Calculate business impact score\n",
    "    business_impact = 0\n",
    "    critical_systems_affected = 0\n",
    "    \n",
    "    for asset_id in affected_assets:\n",
    "        asset = tracker.assets[asset_id]\n",
    "        if asset.risk_level == RiskLevel.CRITICAL:\n",
    "            critical_systems_affected += 1\n",
    "            business_impact += 0.4\n",
    "        elif asset.risk_level == RiskLevel.HIGH:\n",
    "            business_impact += 0.2\n",
    "        elif asset.risk_level == RiskLevel.MEDIUM:\n",
    "            business_impact += 0.1\n",
    "    \n",
    "    failure_analysis = {\n",
    "        'failed_asset': failed_asset.name,\n",
    "        'total_affected': len(affected_assets),\n",
    "        'critical_systems_affected': critical_systems_affected,\n",
    "        'estimated_business_impact': min(business_impact, 1.0),\n",
    "        'recovery_priority': 'HIGH' if critical_systems_affected > 0 else 'MEDIUM',\n",
    "        'affected_asset_details': [\n",
    "            {\n",
    "                'name': tracker.assets[asset_id].name,\n",
    "                'type': tracker.assets[asset_id].type.value,\n",
    "                'risk_level': tracker.assets[asset_id].risk_level.value\n",
    "            }\n",
    "            for asset_id in affected_assets\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return failure_analysis\n",
    "\n",
    "# Run advanced analytics\n",
    "print('🔬 ADVANCED DDI ANALYTICS')\n",
    "print('=' * 40)\n",
    "\n",
    "centrality_analysis = analyze_data_centrality(ddi_tracker)\n",
    "\n",
    "print('\\n💥 CASCADING FAILURE ANALYSIS:')\n",
    "failure_scenarios = ['customer_transactions', 'fraud_detection_model']\n",
    "\n",
    "for scenario_asset in failure_scenarios:\n",
    "    if scenario_asset in ddi_tracker.assets:\n",
    "        analysis = simulate_cascading_failure(ddi_tracker, scenario_asset)\n",
    "        if analysis:\n",
    "            print(f'\\n   Scenario: {analysis[\"failed_asset\"]} Failure')\n",
    "            print(f'   - Total assets affected: {analysis[\"total_affected\"]}')\n",
    "            print(f'   - Critical systems affected: {analysis[\"critical_systems_affected\"]}')\n",
    "            print(f'   - Business impact score: {analysis[\"estimated_business_impact\"]:.3f}')\n",
    "            print(f'   - Recovery priority: {analysis[\"recovery_priority\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Interactive Exploration\n",
    "\n",
    "Now it's your turn to explore and experiment with DDI concepts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 EXPERIMENT: Add your own data asset and see how it affects the system\n",
    "\n",
    "print('🧪 ADD YOUR OWN DATA ASSET:')\n",
    "\n",
    "# Create a new asset - modify these values to experiment\n",
    "your_asset = DataAsset(\n",
    "    id=\"my_experimental_asset\",\n",
    "    name=\"My Experimental Data Asset\",\n",
    "    type=DataAssetType.SOURCE,  # Try: SOURCE, TRANSFORMATION, MODEL, OUTPUT\n",
    "    source_system=\"My System\",\n",
    "    created_at=datetime.now() - timedelta(days=1),\n",
    "    last_updated=datetime.now(),\n",
    "    quality_score=0.75,  # Try different values: 0.0 to 1.0\n",
    "    risk_level=RiskLevel.MEDIUM,  # Try: LOW, MEDIUM, HIGH, CRITICAL\n",
    "    metadata={\"experiment\": True}\n",
    ")\n",
    "\n",
    "# Add to tracker\n",
    "ddi_tracker.add_asset(your_asset)\n",
    "\n",
    "# Connect it to existing assets (experiment with different connections)\n",
    "# ddi_tracker.add_dependency(\"my_experimental_asset\", \"processed_transactions\", \"my_transformation\", 0.8)\n",
    "\n",
    "print(f'✅ Added: {your_asset.name}')\n",
    "print(f'   Risk Level: {your_asset.risk_level.value}')\n",
    "print(f'   Quality Score: {your_asset.quality_score}')\n",
    "\n",
    "# Re-run analysis with your new asset\n",
    "updated_report = ddi_tracker.generate_lineage_report()\n",
    "print(f'\\n📊 Updated System Stats:')\n",
    "print(f'   Total Assets: {updated_report[\"total_assets\"]}')\n",
    "print(f'   Average Quality: {updated_report[\"quality_summary\"][\"average_quality\"]:.3f}')\n",
    "\n",
    "# What changes did your asset make to the system?\n",
    "print('\\n🤔 Discussion Questions:')\n",
    "print('1. How did adding your asset affect the overall system quality?')\n",
    "print('2. What would happen if your asset had a quality issue?')\n",
    "print('3. How would you monitor this asset in production?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Real-World Implementation Strategies\n",
    "\n",
    "Let's discuss practical implementation approaches for DDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🏢 REAL-WORLD DDI IMPLEMENTATION STRATEGIES')\n",
    "print('=' * 50)\n",
    "\n",
    "implementation_phases = {\n",
    "    'Phase 1: Foundation (Months 1-2)': [\n",
    "        '• Inventory all data assets and systems',\n",
    "        '• Establish data quality metrics and thresholds',\n",
    "        '• Implement basic lineage tracking',\n",
    "        '• Set up monitoring infrastructure'\n",
    "    ],\n",
    "    'Phase 2: Intelligence (Months 3-4)': [\n",
    "        '• Deploy automated quality assessment',\n",
    "        '• Build risk scoring algorithms',\n",
    "        '• Create impact analysis capabilities',\n",
    "        '• Establish alerting and notification systems'\n",
    "    ],\n",
    "    'Phase 3: Proactive Governance (Months 5-6)': [\n",
    "        '• Implement predictive quality monitoring',\n",
    "        '• Deploy automated remediation workflows',\n",
    "        '• Build governance dashboards',\n",
    "        '• Establish governance policies and procedures'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for phase, tasks in implementation_phases.items():\n",
    "    print(f'\\n📅 {phase}')\n",
    "    for task in tasks:\n",
    "        print(f'   {task}')\n",
    "\n",
    "print('\\n🛠️ TECHNICAL ARCHITECTURE COMPONENTS:')\n",
    "architecture_components = {\n",
    "    'Data Catalog': 'Apache Atlas, DataHub, or AWS Glue Data Catalog',\n",
    "    'Quality Engine': 'Great Expectations, Deequ, or custom Python validators',\n",
    "    'Lineage Tracking': 'OpenLineage, DataHub, or custom graph database',\n",
    "    'Monitoring': 'Prometheus + Grafana, DataDog, or custom dashboards',\n",
    "    'Workflow Orchestration': 'Apache Airflow, Prefect, or AWS Step Functions',\n",
    "    'Storage': 'PostgreSQL for metadata, Redis for caching, S3 for artifacts'\n",
    "}\n",
    "\n",
    "for component, options in architecture_components.items():\n",
    "    print(f'   {component}: {options}')\n",
    "\n",
    "print('\\n⚠️ COMMON IMPLEMENTATION CHALLENGES:')\n",
    "challenges_and_solutions = {\n",
    "    'Data Silos': 'Use APIs and standardized metadata formats for integration',\n",
    "    'Scale': 'Implement sampling strategies and distributed processing',\n",
    "    'Legacy Systems': 'Start with new systems, gradually extend to legacy',\n",
    "    'Cultural Resistance': 'Demonstrate value early with quick wins',\n",
    "    'Resource Constraints': 'Begin with critical paths, expand incrementally'\n",
    "}\n",
    "\n",
    "for challenge, solution in challenges_and_solutions.items():\n",
    "    print(f'   • {challenge}: {solution}')\n",
    "\n",
    "print('\\n📊 SUCCESS METRICS TO TRACK:')\n",
    "success_metrics = [\n",
    "    'Mean Time to Detection (MTTD) for data quality issues',\n",
    "    'Mean Time to Resolution (MTTR) for data incidents',\n",
    "    'Percentage of data assets with quality scores > 90%',\n",
    "    'Number of prevented downstream incidents',\n",
    "    'Reduction in model performance degradation events',\n",
    "    'Stakeholder satisfaction with data reliability'\n",
    "]\n",
    "\n",
    "for i, metric in enumerate(success_metrics, 1):\n",
    "    print(f'   {i}. {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Summary and Next Steps\n",
    "\n",
    "Let's wrap up with key takeaways and actionable next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🎓 DYNAMIC DATA INTELLIGENCE WORKSHOP SUMMARY')\n",
    "print('=' * 55)\n",
    "\n",
    "print('\\n🎯 KEY TAKEAWAYS:')\n",
    "takeaways = [\n",
    "    'Data lineage is essential for understanding system dependencies',\n",
    "    'Quality issues propagate and amplify through connected systems',\n",
    "    'Proactive monitoring prevents small issues from becoming big problems',\n",
    "    'Risk assessment must consider both direct and indirect impacts',\n",
    "    'Graph-based analysis reveals hidden system vulnerabilities',\n",
    "    'Automation is key to scaling data governance efforts'\n",
    "]\n",
    "\n",
    "for i, takeaway in enumerate(takeaways, 1):\n",
    "    print(f'   {i}. {takeaway}')\n",
    "\n",
    "print('\\n🛠️ TOOLS AND TECHNIQUES EXPLORED:')\n",
    "techniques = [\n",
    "    '✅ Data asset inventory and classification',\n",
    "    '✅ Automated lineage tracking with NetworkX',\n",
    "    '✅ Risk propagation analysis',\n",
    "    '✅ Dynamic quality monitoring',\n",
    "    '✅ Centrality analysis for critical path identification',\n",
    "    '✅ Cascading failure simulation',\n",
    "    '✅ Proactive governance recommendations'\n",
    "]\n",
    "\n",
    "for technique in techniques:\n",
    "    print(f'   {technique}')\n",
    "\n",
    "print('\\n🚀 IMMEDIATE NEXT STEPS (This Week):')\n",
    "immediate_steps = [\n",
    "    'Inventory your critical data assets and AI models',\n",
    "    'Identify your highest-risk data dependencies',\n",
    "    'Set up basic quality monitoring for key datasets',\n",
    "    'Create a simple lineage map of your most critical AI workflow'\n",
    "]\n",
    "\n",
    "for i, step in enumerate(immediate_steps, 1):\n",
    "    print(f'   {i}. {step}')\n",
    "\n",
    "print('\\n📅 LONGER-TERM ACTIONS (Next Month):')\n",
    "longterm_actions = [\n",
    "    'Implement automated lineage tracking in your data pipelines',\n",
    "    'Build a data quality dashboard for stakeholders',\n",
    "    'Establish data quality SLAs and alerting',\n",
    "    'Create incident response procedures for data quality issues',\n",
    "    'Train your team on DDI principles and tools'\n",
    "]\n",
    "\n",
    "for i, action in enumerate(longterm_actions, 1):\n",
    "    print(f'   {i}. {action}')\n",
    "\n",
    "print('\\n🌐 RESOURCES FOR CONTINUED LEARNING:')\n",
    "resources = [\n",
    "    'Great Expectations documentation: https://docs.greatexpectations.io/',\n",
    "    'Apache Atlas for data governance: https://atlas.apache.org/',\n",
    "    'OpenLineage for lineage tracking: https://openlineage.io/',\n",
    "    'DataHub for data discovery: https://datahubproject.io/',\n",
    "    'This workshop repository with all examples and code'\n",
    "]\n",
    "\n",
    "for resource in resources:\n",
    "    print(f'   • {resource}')\n",
    "\n",
    "print('\\n💬 DISCUSSION QUESTIONS:')\n",
    "discussion_questions = [\n",
    "    'What was the most surprising insight from the workshop?',\n",
    "    'Which DDI technique would have the biggest impact in your organization?',\n",
    "    'What are the biggest barriers to implementing DDI in your environment?',\n",
    "    'How would you measure ROI for a DDI implementation?',\n",
    "    'What governance challenges are unique to AI/ML systems?'\n",
    "]\n",
    "\n",
    "for i, question in enumerate(discussion_questions, 1):\n",
    "    print(f'   {i}. {question}')\n",
    "\n",
    "print('\\n🎉 Thank you for participating in the Dynamic Data Intelligence workshop!')\n",
    "print('   Remember: Great AI starts with great data governance.')\n",
    "print('   The future of AI depends on our ability to understand and trust our data.')\n",
    "\n",
    "# Final system stats\n",
    "final_report = ddi_tracker.generate_lineage_report()\n",
    "print(f'\\n📊 Final Workshop Stats:')\n",
    "print(f'   Assets Created: {final_report[\"total_assets\"]}')\n",
    "print(f'   Dependencies Mapped: {final_report[\"total_dependencies\"]}')\n",
    "print(f'   Quality Assessments Run: {len(quality_results) if \"quality_results\" in locals() else 0}')\n",
    "print(f'   Risk Analyses Performed: Multiple scenarios covered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done exploring, clean up the Docker containers:\n",
    "\n",
    "```bash\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Thank you for participating in the Dynamic Data Intelligence workshop!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
